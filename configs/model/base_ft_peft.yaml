peft_config:
  r: 16
  lora_alpha: 64
  target_modules:
    - "^llm.model.layers.[0-9]+.self_attn.q_proj$"
    - "^llm.model.layers.[0-9]+.self_attn.k_proj$"
    - "^llm.model.layers.[0-9]+.self_attn.v_proj$"
    - "^llm.model.layers.[0-9]+.self_attn.o_proj$"
  bias: none

unfreeze_embedding: False

checkpoint_dir: outputs/pretrain_adapter/2025-10-16_19-24-55/checkpoint-92000
